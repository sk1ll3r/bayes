\section{Importance sampling}

Assume we can sample from a proposal distribution $q$ with a pdf $q(\vec x)$, which can be evaluated within a multiplicative factor (i.e. we can only evaluate $q^\ast(\vec x) = Z_q q(\vec x)$). To solve problem 2, we follow Algorithm~\ref{alg:importance} below.
\begin{algorithm}
\caption{Importance sampling}\label{alg:importance}
    \begin{algorithmic}[1]
        \State Generate samples from $q$, $\{\vec x^{(r)}\}$.
        \State Calculate importance weights $w_r = \frac{p^\ast(\vec x^{(r)})}{q^\ast(\vec x^{(r)})}$.
        \State $\hat{\vec y} = \frac{\sum_r w_r f(\vec x^{(r)})}{\sum_r w_r}$ is the estimator of $\E[f]$.
    \end{algorithmic}
\end{algorithm}

\subsection{Convergence of estimator as $R$ increases}
    We want to prove that if $q(\vec x)$ is non-zero for all $\vec x$ where $p(\vec x)$ is non-zero, the estimator $\hat{\vec y}$ converges to $\E[f]$, as $R$ increases. We consider the the expectations of the numerator and denominator separately:
    \begin{align}
        \E_q[\text{numer}]  &= \E_q\left[\sum_r w_r f(\vec x^{(r)})\right] \\
                            &= \sum_r \E_q\left[w_r f(\vec x^{(r)})\right] \\
                            &= \sum_r \E_q\left[\frac{p^\ast(\vec x^{(r)})}{q^\ast(\vec x^{(r)})} f(\vec x^{(r)})\right] \\
                            &= \sum_r \E_q\left[\frac{Z_p p(\vec x^{(r)})}{Z_q q(\vec x^{(r)})} f(\vec x^{(r)})\right] \\
                            &= \frac{Z_p}{Z_q} \sum_r \int_{\vec x^{(r)}} p(\vec x^{(r)}) f(\vec x^{(r)}) \, \mathrm d \vec x^{(r)} \\
                            &= \frac{Z_p}{Z_q} \sum_r \E_p\left[f(\vec x^{(r)})\right] \\
                            &= \frac{Z_p}{Z_q} R \E_p\left[f(\vec x)\right] \\
        \E_q[\text{denom}]  &= \E_q\left[\sum_r w_r\right] \\
                            &= \sum_r \E_q\left[\frac{p^\ast(\vec x^{(r)})}{q^\ast(\vec x^{(r)})}\right] \\
                            &= \sum_r \E_q\left[\frac{Z_p p(\vec x^{(r)})}{Z_q q(\vec x^{(r)})}\right] \\
                            &= \frac{Z_p}{Z_q} \sum_r \int_{\vec x^{(r)}} p(\vec x^{(r)}) \,\mathrm d \vec x^{(r)} \\
                            &= \frac{Z_p}{Z_q} R
    \end{align}

    Hence $\hat{\vec y}$ converges to $\E_p[f]$ as $R$ increases (but is not necessarily an unbiased estimator because $\E_q[\hat{\vec y}]$ is not necessarily $= \E_p[f]$).

\subsection{Optimal proposal distribution}
    Assuming we can evaluate $p(\vec x)$ and $q(\vec x)$, we want to find a proposal distribution $q$ to minimise the variance of the weighted samples
    \begin{align}
        \var_q\left[\frac{p(\vec x)}{q(\vec x)} f(\vec x)\right]    &= \E_q\left[\frac{p^2(\vec x)}{q^2(\vec x)} f^2(\vec x)\right] - \left( \E_q\left[\frac{p(\vec x)}{q(\vec x)} f(\vec x)\right] \right)^2 \\
                                                                    &= \E_q\left[\frac{p^2(\vec x)}{q^2(\vec x)} f^2(\vec x)\right] - \left( \E_p\left[f(\vec x)\right] \right)^2
    \end{align}

    The second part is independent of $q$ so we can ignore it. By Jensen's inequality, we have $\E\left[g(u(\vec x))\right] \geq g\left(\E\left[u(\vec x\right)]\right)$ for $u(\vec x) \geq 0$ where $g: x \mapsto x^2$. Setting $u(\vec x) = p(\vec x)|f(\vec x)| / q(\vec x)$, we have the following lower bound:
    \begin{equation}
        \E_q\left[\frac{p^2(\vec x)}{q^2(\vec x)} f^2(\vec x)\right] \geq \left( \E_q\left[\frac{p(\vec x)}{q(\vec x)} |f(\vec x)|\right] \right)^2 = \left( \E_p[|f(\vec x)|] \right)^2
    \end{equation}
    with the equality when $u(\vec x) = \text{const.} \implies q_{\text{optimal}}(\vec x) \propto |f(\vec x)|p(\vec x)$. Taking care of normalisation, we get
    \begin{equation}
        q_{\text{optimal}}(\vec x) = \frac{|f(\vec x)|p(\vec x)}{\int |f(\vec x')|p(\vec x') \,\mathrm d \vec x'}
    \end{equation}