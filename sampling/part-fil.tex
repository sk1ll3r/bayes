\section{Particle filtering}
\subsection{Sequential importance sampling (SIS)}
    Assume the probabilistic graphical model similar to the one in HMMs, where $\vec x_t$ and $\vec y_t$ are the hidden and observed random variables at time $t$ respectively. We want to sample from the distribution $p(\vec x_{1:t} \mid \vec y_{1:t})$. Assume we can sample from the probability distribution with the pdf of the following form
    \begin{align}
        q(\vec x_{1:t} \mid \vec y_{1:t})   &= q(\vec x_t \mid \vec x_{1:t - 1}, \vec y_{1:t}) q(\vec x_{1:t-1} \mid \vec y_{1:t}) \\
                                            &= q(\vec x_t \mid \vec x_{1:t - 1}, \vec y_{1:t}) q(\vec x_{1:t-1} \mid \vec y_{1:t - 1}) \\
                                            &= q(\vec x_t \mid \vec x_{t - 1}, \vec y_t)
    \end{align}
    If we express the pdf of $p$ in the form of
    \begin{align}
        p(\vec x_{1:t} \mid \vec y_{1:t})   &= \frac{p(\vec y_{1:t} \mid \vec x_{1:t}) p(\vec x_{1:t})}{p(\vec y_{1:t})} \\
                                            &= \frac{p(\vec y_t \mid \vec x_{1:t}, \vec y_{1:t - 1}) p(\vec y_{1:t - 1} \mid \vec x_{1:t}) p(\vec x_{1:t})}{p(\vec y_t \mid \vec y_{1:t - 1}) p(\vec y_{1:t - 1})} \\
                                            &= \frac{p(\vec y_t \mid \vec x_{1:t}, \vec y_{1:t - 1}) p(\vec x_{1:t} \mid \vec y_{1:t - 1})}{p(\vec y_t \mid \vec y_{1:t - 1})} \\
                                            &= \frac{p(\vec y_t \mid \vec x_{1:t}, \vec y_{1:t - 1}) p(\vec x_t \mid \vec x_{1:t - 1}, \vec y_{1:t - 1}) p(\vec x_{1:t - 1} \mid \vec y_{1:t - 1})}{p(\vec y_t \mid \vec y_{1:t - 1})} \\
                                            &= \frac{p(\vec y_t \mid \vec x_t) p(\vec x_t \mid \vec x_{t - 1}) p(\vec x_{1:t - 1} \mid \vec y_{1:t - 1})}{p(\vec y_t \mid \vec y_{1:t - 1})} \\
                                            &\propto p(\vec y_t \mid \vec x_t) p(\vec x_t \mid \vec x_{t - 1}) p(\vec x_{1:t - 1} \mid \vec y_{1:t - 1})
    \end{align}
    we can write the weight of the sample $\vec x_{1:t}^{(r)}$ from the proposal $q$ to be
    \begin{align}
        w_t^{(r)}   &\propto \frac{p\left(\vec x_{1:t}^{(r)} \mid \vec y_{1:t}\right)}{q\left(\vec x_{1:t}^{(r)} \mid \vec y_{1:t}\right)} \\
                    &\propto \frac{p\left(\vec y_t \mid \vec x_t^{(r)}\right) p\left(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}\right) p\left(\vec x_{1:t - 1}^{(r)} \mid \vec y_{1:t - 1}\right)}{q\left(\vec x_t^{(r)} \mid \vec x_{t-1}^{(r)}, \vec y_t\right) q\left(\vec x_{1:t-1}^{(r)} \mid \vec y_{1:t - 1}\right)} \\
                    &= w_{t - 1}^{(r)} \frac{p\left(\vec y_t \mid \vec x_t^{(r)}\right) p\left(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}\right)}{q\left(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}, \vec y_t\right)} \label{eq:weights}
    \end{align}
    The algorithm for SIS is shown in Algorithm~\ref{alg:sis} below.
    \begin{algorithm}
    \caption{Sequential importance sampling}\label{alg:sis}
        \begin{algorithmic}[1]
            \State Initialise weights $\left\{w_0^{(r)} = \frac{1}{R}\right\}$.
            \For{$t = 1, \dotsc, T$}
                \State Observe $\vec y_t$.
                \State Sample $\left\{\vec x_{1:t}^{(r)}\right\}$ from $q\left(\vec x_{1:t} \mid \vec y_{1:t}\right)$.
                \State Calculate weights $\left\{w_t^{(r)}\right\}$ according to \eqref{eq:weights}.
                \State Calculate normalised weights $\left\{\hat w_t^{(r)} = \frac{w_t^{(r)}}{\sum_{r'} w_t^{(r')}}\right\}$.
                \State \Comment{The pmf $\sum_r \hat w_t^{(r)} \delta_{\vec x_{1:t}^{(r)}}(\vec x_{1:t})$ approximates the pdf $p(\vec x_{1:t} \mid \vec y_{1:t})$. Hence we can approximate the pdf $p(\vec x_t \mid \vec y_{1:t})$ by $\sum_r \hat w_t^{(r)} \delta_{\vec x_t^{(r)}}(\vec x_t)$.}
            \EndFor
        \end{algorithmic}
    \end{algorithm}
    The reason why it works is the same as in the case of Sampling importance resampling described in section \ref{section:sir}.

\subsection{The degeneracy problem}
    Because the support of the pdf we are approximating ($p(\vec x_{1:t} \mid \vec y_{1:t})$) is growing, the constant number of weights we use ($R$) won't be sufficient after a while. This is because many weights will become very negligible, wasting our resources. An \textbf{effective sample size} is used to measure this degeneracy is defined to be and approximated by the following:
    \begin{align}
        S_\text{eff}        &\triangleq \frac{S}{1 + \var\left[{w_t^{(r)}}^\ast\right]} \\
        \hat S_\text{eff}   &\approx \frac{1}{\sum_r \left(w_t^{(r)}\right)^2}\label{eq:Seff}
    \end{align}
    where ${w_t^{(r)}}^\ast = p(\vec x_t^{(r)} \mid \vec y_{1:t}) / q(\vec x_t^{(r)} \mid \vec x_{t-1}^{(r)}, \vec y_t)$ is the ``true weight'' of particle $r$.

    There are (among others) two solutions to this problem -- introduce the resampling step, and using a good proposal distribution.

\subsection{The resampling step}
    Whenever the effective sample size drops below some threshold, resample to get new $R$ samples from the approximation of the pdf. This step is also called \textbf{rejuvenation}. The full algorithm for a generic particle filter is shown in Algorithm~\ref{alg:part-fil} below.
    \begin{algorithm}
    \caption{Generic particle filter}\label{alg:part-fil}
        \begin{algorithmic}[1]
            \State Initialise weights $\left\{w_0^{(r)} = \frac{1}{R}\right\}$.
            \For{$t = 1, \dotsc, T$}
                \State Observe $\vec y_t$.
                \State Sample $\left\{\vec x_{1:t}^{(r)}\right\}$ from $q\left(\vec x_{1:t} \mid \vec y_{1:t}\right)$.
                \State Calculate weights $\left\{w_t^{(r)}\right\}$ according to \eqref{eq:weights}.
                \State Calculate normalised weights $\left\{\hat w_t^{(r)} = \frac{w_t^{(r)}}{\sum_{r'} w_t^{(r')}}\right\}$.
                \State Calculate the effective sample size, $\hat S_\text{eff}$, according to \eqref{eq:Seff}.
                \If{$\hat S_\text{eff} < S_\text{min}$}
                    \State Resample $R$ particles, $\left\{\vec x_t^{(r)}\right\}$ from the pmf $\sum_r \hat w_t^{(r)} \delta_{\vec x_t^{(r)}}(\vec x_t)$.
                    \State Reassign $w_t^{(r)} = \frac{1}{R}$ for $r = 1, \dotsc, R$.
                \EndIf
            \EndFor
        \end{algorithmic}
    \end{algorithm}

\subsection{Particle filter animation}
\animategraphics[controls, width=1\linewidth]{0.3}{animations/particle-filter/}{1}{36}

\subsection{The proposal distribution}
    It is common to use the following proposal distribution
    \begin{align}
        q(\vec x_{1:t}^{(r)} \mid \vec y_{1:t})     &= q(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}, \vec y_t) \\
                                                    &= p(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)})
    \end{align}
    Hence the weight equation in \eqref{eq:weights} becomes
    \begin{align}
        w_t^{(r)}   &\propto w_{t - 1}^{(r)} \frac{p\left(\vec y_t \mid \vec x_t^{(r)}\right) p\left(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}\right)}{q\left(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}, \vec y_t\right)} \\
                    &= w_{t - 1}^{(r)} p\left(\vec y_t \mid \vec x_t^{(r)}\right)
    \end{align}
    This approach can be \textbf inefficient because the likelihood, $p\left(\vec y_t \mid \vec x_t^{(r)}\right)$, can be very small at many places meaning many of the particles will be very small.

    The optimal proposal distribution has the form
    \begin{align}
        q(\vec x_{1:t}^{(r)} \mid \vec y_{1:t})     &= q(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}, \vec y_t) \\
                                                    &= p(\vec x_t^{(r)} \mid \vec x_{t - 1}^{(r)}, \vec y_t) \\
                                                    &= \frac{p(\vec y_t \mid \vec x_t, \vec x_{t-1}^{(r)}) p(\vec x_t, \vec x_{t-1}^{(r)})}{p(\vec x_{t-1}^{(r)}, \vec y_t)} \\
                                                    &= \frac{p(\vec y_t \mid \vec x_t) p(\vec x_t \mid \vec x_{t-1}^{(r)})}{p(\vec y_t \mid \vec x_{t-1}^{(r)})}
    \end{align}
    The weight equation in \eqref{eq:weights} becomes
    \begin{align}
        w_t^{(r)}   &\propto w_{t - 1}^{(r)} p\left(\vec y_t \mid \vec x_{t-1}^{(r)}\right) \\
                    &= w_{t - 1}^{(r)} \int{p(\vec y_t, \vec x'_t \mid \vec x_{t-1}^{(r)}) \,\mathrm d\vec x'} \\
                    &= w_{t - 1}^{(r)} \int{p(\vec y_t \mid \vec x'_t, \vec x_{t-1}^{(r)}) p(\vec x'_t \mid \vec x_{t-1}^{(r)}) \,\mathrm d\vec x'} \\
                    &= w_{t - 1}^{(r)} \int{p(\vec y_t \mid \vec x'_t) p(\vec x'_t \mid \vec x_{t-1}^{(r)}) \,\mathrm d\vec x'}
    \end{align}
    The proposal distribution is optimal because for any fixed $\vec x_{t-1}^{(r)}$, the new weight $w_t^{(r)}$ takes the same value regardless of the value drawn for $\vec x_t^{(r)}$. Hence, conditional on the old values, the variance of true weights is zero.